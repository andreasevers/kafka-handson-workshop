<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <title>JWorks - Kafka on Docker</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/ordina.css" id="theme">
    <link rel="stylesheet" href="css/custom.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <script src="js/head.min.js"></script>
    <!--Add support for earlier versions of Internet Explorer -->
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Kafka on Docker</h1>
                <h3>Containerize it all</h3>
                <img src="img/jworks-logo.png" style="vertical-align: bottom;" width="22.5%"/>
            </section>

            <section>
                <h2>Hi, my name is Tom.</h2>
                <p>
                    Developer<br />
                    Ordina Belgium<br />
                    @tomvdbulck<br />
                    https://github.com/tomvdbulck
                </p>
            </section>

            <!-- **********
                EDIT FROM HERE
            ********** -->
            <section>
                <h2>Agenda</h2>
                <ul>
                    <li>Kafka</li>
                    <li>Zookeeper</li>
                    <li>Docker</li>
                    <li>All in One</li>
                    <li>Scale Up</li>
                    <li>Kafka Monitor</li>
                </ul>
            </section>

            <!-- Example of nested vertical slides -->
            <section>
                <section>
                    <h2>Kafka</h2>

                    <img width="50%" src="img/kafka_logo.png">
                </section>
                <section>
                    <h2>Kafka</h2>
                    <img width="50%" src="img/kafka_diagram.png">
                    <aside class="notes"><ul>
                        <li>Publish and Subscribe</li>
                        <li>Process Streams of Data</li>
                        <li>Store</li>
                    </ul></aside>
                </section>
                <section>
                    <h2>4 Core API's</h2>
                    <img width="50%" src="img/kafka-apis.png">
                </section>
                <section>
                    <h2>4 Core API's</h2>
                    <ul>
                        <li>Producer API: publish to a topic</li>
                        <li>Consumer API: subscribe to a topic</li>
                        <li>Streams API: stream processor</li>
                        <li>Connector API: resuable producers and consumers</li>
                    </ul>
                    <aside class="notes">
                        <lu>
                            <li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
                            <li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
                            <li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
                            <li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
                        </lu>
                    </aside>
                </section>
                <section>
                    <h2>Topics and Logs</h2>
                    <img width="50%" src="img/log_anatomy.png">
                    <aside class="notes">
                        A topic is to where records are published - always multi-subscriber (0 - n consumers)
                        
                        For each topic Kafka maintains a partitioned log
                        
                        Each partition is an ordered, immutable sequence of records that is continuously appended to.
                        Each record in a partition is assigned an offset, which uniquely identifies each record within a partition.
                        
                    </aside>
                </section>
                <section>
                    <h2>Topics and Logs</h2>
                    <ul>
                        <li>The cluster retains all published records</li>
                        <li>The consumer controls its offset</li>
                        <li>Low overhead per consumer</li>
                    </ul>
                    <aside class="notes">
                        All records are retained for a configurable time.
                        Only overhead per consumer is the current offset of that consumer.
                    </aside>
                </section>
                <section>
                    <h2>Partitions</h2>
                    <ul>
                        <li>Allow the system to scale.</li>
                        <li>Are replicated</li>
                        <li>Each partition has 1 leader</li>
                        <li>Guarantees order of records</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        Kafka only provides a total order over records within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications.
                    </aside>
                </section>
                <section>
                    <h2>Producers</h2>
                    <ul>
                        <li>Will assign record to a partition</li>
                        <li>Round Robin or based on a key</li>
                    </ul>
                    <aside class="notes">

                    </aside>
                </section>
                <section>
                    <h2>Consumers</h2>
                    <ul>
                        <li>Consumers are part of a Consumer Group</li>
                        <li>1 record published to 1 consumer instance</li>
                        <li>Partitions will determine max. # of consumers</li>
                        <li>Scalable within a Consumer Group</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a "fair share" of partitions at any point in time. This process of maintaining membership in the group is handled by the Kafka protocol dynamically. If new instances join the group they will take over some partitions from other members of the group; if an instance dies, its partitions will be distributed to the remaining instances.
                    </aside>
                </section>
                <section>
                    <h2>Kafka Streams</h2>
                    <img width="50%" src="img/streams-welcome.png">
                    <aside class="notes">
                        Simple and lightweight client library
                        No external dependencies - only to Kafka
                        Exactly-once is supported
                    </aside>
                </section>
                <section>
                    <h2>Kafka Streams</h2>
                    <img width="50%" src="img/streams-architecture-topology.jpg">
                    <aside class="notes">
                        A Stream represents an unbounded, continuously updating data set.
                        An ordered, replayable and fault-tolerant sequence of immutable data records
                        Stream processing application uses one or more processor topologies.
                        Stream processor is a note in the processor topology
                        Source Processor: produces and input stream
                        Sink Processor: has no down-stream processors
                    </aside>
                </section>
                <section>
                    <h2>A Storage System</h2>
                    <ul>
                        <li>Kafka is a very good storage system</li>
                        <li>Written to Disk and Replicated</li>
                        <li>No Performance Loss</li>
                        <li>https://www.confluent.io/blog/okay-store-data-apache-kafka/</li>
                    </ul>
                    <aside class="notes">
                        Any message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages. What is different about Kafka is that it is a very good storage system.
                        Data written to kafka is written to disk and replicated.
                        Allows producers to wait on acknowledgement so that a write is only considered complete when replicated and guaranteed to persist event if the server fails
                        
                        The disk structures Kafka uses scale well—Kafka will perform the same whether you have 50 KB or 50 TB of persistent data on the server.

As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.



                    </aside>
                </section>
                <section>
                    <h2>A Messaging System</h2>
                    <p> Traditionally 2 models
                    <ul>
                        <li>Queueing: process record once</li>
                        <li>Publish and Subscribe: broadcast</li>
                        
                    </ul>
                    </p>
                    <aside class="notes">
                        Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers. Each of these two models has a strength and a weakness. The strength of queuing is that it allows you to divide up the processing of data over multiple consumer instances, which lets you scale your processing. Unfortunately, queues aren't multi-subscriber—once one process reads the data it's gone. Publish-subscribe allows you broadcast data to multiple processes, but has no way of scaling processing since every message goes to every subscriber.
                    </aside>
                </section>
                <section>
                    <h2>The Consumer Group</h2>
                    <p><ul>
                        <li>Divide Processing - like with a queue</li>
                        <li>Broadcast to multiple consumer groups - like publish-subscribe</li>
                        <li>Stronger Ordering Guarantees</li>
                    </ul>
                        
                    </p>
                    
                    <aside class="notes">
                        Paralel processing over members of a consumer group.
                        Broadcast to other consumer groups
                        
                        This for every topic
                        
                        Strong ordering guarantees then traditional systems - queues provide records as received.
                        But if more consumers process then ordering can be lost.
                        Thanks to the partitions this ordering can be retained
                    </aside>
                </section>

            </section>

            <section>
                <section>
                    <h2>Zookeeper</h2>

                    <img width="20%" src="img/zookeeper_logo.jpeg">
                </section>
                <section>
                    <h2>Zookeeper</h2>

                    <img width="50%" src="img/zookeeper%20service.png">
                    <aside class="notes">
                        A Distributed Coordination Service
                        For distributed Applications
                        Data model styled after the familiar tree structure of file systems
                        Easy to program to so that distributed applications can build upon for synchronization, configuration maintenance and groups and naming.
                    </aside>    
                </section>
                <section>
                    <h2>Zookeeper</h2>
                    <ul>
                        <li>Distributed Coordination Service</li>
                        <li>For Distributed Applications</li>
                        <li>File system like Structure</li>
                    </ul>
                </section>
                <section>
                    <h2>Why does Kafka Need it</h2>
                    <p>
                        <ul>
                            <li>Electing a Controller</li>
                            <li>Cluster Membership</li>
                            <li>Topic Configuration</li>
                            <li>Quotas</li>
                            <li>ACL: Who is allowed to read and write</li>
                        </ul>
                    
                    </p>
                <aside class="notes">
                        The controler is one of the brokers, a broker is a kafka node, which is is responsible to maintain the leader/follower relationship for all Partitions. Zookeepers makes sure there is 1 and if it crashed will elect a new one. The controller will tell replicas to become partition leaders - can reshuffle partitions, ... 
                    
                        Cluster membership: which brokers are alive and part of the cluster.
                    
                        Which topics exist, how many partitions, where are the replicas, ... 
                        
                        How much data is each client allowed to read and write
                        
                        Kafka comes with Kafka Authorization Command Line - define uses and access
                    </aside>
                </section>
            </section>

            <section>
                <section>
                    <h2>Docker</h2>

                    <img width="50%" src="img/docker-horizontal.png">
                </section>
                <section>
                    <h2>Docker</h2>
                    <ul>
                        <li>Build: standardized way using Docker CLI in a DockerFile</li>
                        <li>Ship: share the container - it is just a file</li>
                        <li>Run: less overhead then a VM</li>
                    </ul>
                    <aside class="notes">
                        Docker runs on a minimal operation system so it has much less overhead then a traditional VM.
                    </aside>
                </section>
                <section>
                    <h2>Docker Compose</h2>
                    <ul>
                        <li>Multi-container applications</li>
                        <li>docker-compose.yml</li>
                        <li>Software defined network</li>
                    </ul>
                    <aside class="notes">
                        Defined in a docker-compose file: great tool for setting up development, staging and CI environments.
                        
                        Docker-compose helps you by linking these containers.
                    </aside>
                </section>
            </section>

            <section>
                <section>
                    <h2>1 Container</h2>

                    <img width="50%" src="img/lonely_container.JPG">
                </section>
                <section>
                    <h2>The Goal</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <p>Why?</p> 
                </section>
                <section>
                    <h2>DockerFile</h2>
                    <img width="100%" src="img/single/download-smaller.png">
                    <aside class="notes">
                        Based on the Spotify Kafka Docker image.
                    </aside>
                </section>
                <section>
                    <h2>DockerFile</h2>
                    <img width="100%" src="img/single/get-scripts.png">
                </section>
                <section>
                    <h2>Startup Scripts</h2>
                     <img width="100%" src="img/single/start-zookeeper.png">
                </section>
                <section>
                    <h2>Startup Scripts</h2>
                     <img width="100%" src="img/single/start-kafka.png">
                </section>
                <section>
                    <h2>Get Started: Start the Container</h2>
                    <p>Create the image</p>
                     <pre><code data-trim contenteditable>
                     docker build -t kafka-zookeeper .
                     </code></pre>
                    <p>Startup the container</p>
                    <pre><code data-trim contenteditable>
                     docker run --name a_name -i -t -p 2181:2181 -p 9092:9092 kafka-zookeeper
                     </code></pre>
                </section>
                <section>
                    <h2>Get Started: Start the Container</h2>
                    <p>Access Kafka from outside the container: add the following parameters</p>
                     <pre><code data-trim contenteditable>
                     -e ADVERTISED_HOST=localhost -e ADVERTISED_PORT=9092
                     </code></pre>
                </section>
                <section>
                    <h2>Get Started: Topics</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --create --zookeeper localhost:2181 
                    --replication-factor 1 --partitions 1 --topic test
                     </code></pre>
                    <p>Retrieve a list of topics on Zookeeper</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --list --zookeeper localhost:2181
                     </code></pre>
                    <p>Describe a topic: leader / replicated nodes</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --describe --zookeeper localhost:2181
                     </code></pre>
                </section>
                <section>
                    <h2>Get Started: Produce and Consume</h2>
                    <p>Produce</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
                     </code></pre>
                    <p>Consume</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
                    --topic test --from-beginning
                     </code></pre>
                </section>
            </section>

            <section>
                <section>
                    <h2>Scale Up</h2>

                    <img width="50%" src="img/blue-containers.jpg">
                </section>
                <section>
                    <h2>Origin</h2>
                    <p>Nested slides are useful for adding additional detail underneath a high level horizontal slide.</p>
                </section>
            </section>

            <section>
                <section>
                    <h2>Kafka Monitor</h2>

                </section>
                <section>
                    <h2>Origin</h2>
                    <p>Nested slides are useful for adding additional detail underneath a high level horizontal slide.</p>
                </section>
            </section>

            <section>
                <section>
                    <h2>Questions ?</h2>

                </section>
            </section>




            <!-- **********
                DO NOT REMOVE
            ********** -->
            <section style="text-align: left;">
                <h2>Thanks for watching!</h2>
                <p class="fragment">Now kick some ass!</p>
            </section>

            <section style="text-align: left;" data-background="img/jworks-wallpaper-3.jpg"></section>
        </div>
    </div>
    <script src="js/reveal.js"></script>
    <script>
        Reveal.initialize({
            transition: 'convex',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                {
                    src: 'lib/js/classList.js',
                    condition: function () {
                        return !document.body.classList;
                    }
                },
                // Interpret Markdown in <section> elements
                {
                    src: 'plugin/markdown/marked.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                {
                    src: 'plugin/markdown/markdown.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                // Syntax highlight for <code> elements
                {
                    src: 'plugin/highlight/highlight.js',
                    async: true,
                    callback: function () {
                        hljs.initHighlightingOnLoad();
                    }
                },
                // Zoom in and out with Alt+click
                {
                    src: 'plugin/zoom-js/zoom.js',
                    async: true
                },
                // Speaker notes
                {
                    src: 'plugin/notes/notes.js',
                    async: true
                }
        ]
        });
    </script>
</body>

</html>

<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <title>JWorks - Kafka DIY</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/ordina.css" id="theme">
    <link rel="stylesheet" href="css/custom.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <script src="js/head.min.js"></script>
    <!--Add support for earlier versions of Internet Explorer -->
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Kafka</h1>
                <h3>Do It Yourself</h3>
                <img src="img/jworks-logo.png" style="vertical-align: bottom;" width="22.5%"/>
            </section>

            <section>
                <h2>Hi, my name is Tom.</h2>
                <p>
                    Developer<br />
                    Ordina Belgium<br />
                    @tomvdbulck<br />
                    https://github.com/tomvdbulck
                </p>
            </section>

            <!-- **********
                EDIT FROM HERE
            ********** -->
            <section>
                <h2>Agenda</h2>
                <ul>
                    <li>What is Kafka</li>
                    <li>A Kafka Cluster</li>
                    <li>Kafka: When to Use it</li>
                    <li>Kafka: 4 core API's</li>
                    <li>Hands-On</li>
                </ul>
            </section>

            <!-- Example of nested vertical slides -->
            <section>
                <section>
                    <h2>Kafka</h2>

                    <img width="50%" src="img/kafka_logo.png">
                </section>
                <section>
                    <h2>Kafka</h2>
                    <img width="50%" src="img/kafka_diagram.png">
                    <aside class="notes"><ul>
                        <li>Messaging</li>
                        <li>Storage data - event log</li>
                        <li>Store</li>
                    </ul></aside>
                </section>
                
                
                
                
                <section>
                    <h2>Kafka: A Messaging System</h2>
                    <p> Traditionally 2 models
                    <ul>
                        <li>Queueing: process record once</li>
                        <li>Publish and Subscribe: broadcast</li>
                        
                    </ul>
                    </p>
                    <aside class="notes">
                        Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers. Each of these two models has a strength and a weakness. The strength of queuing is that it allows you to divide up the processing of data over multiple consumer instances, which lets you scale your processing. Unfortunately, queues aren't multi-subscriber—once one process reads the data it's gone. Publish-subscribe allows you broadcast data to multiple processes, but has no way of scaling processing since every message goes to every subscriber.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: A Storage System</h2>
                    <ul>
                        <li>Kafka is a very good storage system</li>
                        <li>Written to Disk and Replicated</li>
                        <li>No Performance Loss</li>
                        <li>https://www.confluent.io/blog/okay-store-data-apache-kafka/</li>
                    </ul>
                    <aside class="notes">
                        Any message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages. What is different about Kafka is that it is a very good storage system.
                        Data written to kafka is written to disk and replicated.
                        Allows producers to wait on acknowledgement so that a write is only considered complete when replicated and guaranteed to persist event if the server fails
                        
                        The disk structures Kafka uses scale well—Kafka will perform the same whether you have 50 KB or 50 TB of persistent data on the server.

As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.
                    </aside>
                </section>
            </section>
            <section>
                <section>
                    <h2>A Kafka Cluster</h2>
                <p>
                    <ul>
                        <li>Zookeeper</li>
                        <li>Broker</li>
                        <li>Controller</li>
                        <li>Topics</li>
                        <li>Partitions</li>
                        <li>Replication</li>
                        <li>Producers</li>
                        <li>Consumers</li>
                        <li>Consumer offset</li>
                        <li>Failover</li>
                    </ul>
                </p>
                
                </section>
                
        
                <section>
                    <h2>Zookeeper</h2>

                    <img width="20%" src="img/zookeeper_logo.jpeg">
                    
                    <aside class="notes">
                        A Distributed Coordination Service
                        For distributed Applications
                        Data model styled after the familiar tree structure of file systems
                        Easy to program to so that distributed applications can build upon for synchronization, configuration maintenance and groups and naming.
                    </aside>   
                </section>
                <section>
                    <h2>Zookeeper: Why does Kafka Need it</h2>
                    <p>
                        <ul>
                            <li>Electing a Controller</li>
                            <li>Cluster Membership</li>
                            <li>Topic Configuration</li>
                            <li>Quotas</li>
                            <li>ACL: Who is allowed to read and write</li>
                        </ul>
                    
                    </p>
                <aside class="notes">
                        Cluster membership: which brokers are alive and part of the cluster.
                    
                        Which topics exist, how many partitions, where are the replicas, ... 
                        
                        How much data is each client allowed to read and write
                        
                        Kafka comes with Kafka Authorization Command Line - define uses and access
                    </aside>
                </section>
                <section>
                    <h2>Brokers</h2>   
                    <p>
                        Node, Broker, Kafka Server => all the same
                    </p>
                    <p>
                        Hosts topics<br/>
                        Stores messages
                    </p>
                    <aside class="notes">
                        Stores messages on disk by a unique offset
                        Shares information between other brokers and zookeeper
                    </aside>
                </section>
                <section>
                    <h2>The Controller</h2>  
                    <p>Manage State of Partitions and Replicas</p>
                    <p>Reassign Partitions</p>
            
                    <aside class="notes">
                         The controler is one of the brokers, a broker is a kafka node, which is is responsible to maintain the leader/follower relationship for all Partitions. Zookeepers makes sure there is 1 and if it crashed will elect a new one. The controller will tell replicas to become partition leaders - can reshuffle partitions, ... 
                        
                        see: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Internals
                    </aside>
                </section>
                <section>
                    <h2>Topics</h2>  
                    <p>
                        Virtual Group of partitions
                    </p>
                    <p>
                        Producers write messages.
                        Consumes read messages.
                    </p>
                </section>
                <section>
                    <h2>Partitions</h2>    
                    <p>
                        Topics are split in partitions.<br/>
                        1 broker is leader.
                    </p>
                    <p>
                        Used to scale.
                    </p>
                    <aside class="notes">
                        Producers and Consumer only speak with the leader.
                        
                        A list of insync replica's is maintained by the brokers / partition.
                        New leader is elected if required.
                        
                        Used to scale up or down the cluster.
                        As consumers and producers approach the cluster via partitions.
                    </aside>
                </section>
                <section>
                    <h2>Partitions: Rebalancing</h2>    
                    <p> auto.leader.rebalance.enable: true (default) </p>
                        
                    <p>    also manual rebalancing possible
                    </p>
                    <aside class="notes">
                    
                        kafka-preferred-replica-election.sh tool
                        
                        see: https://docs.confluent.io/current/kafka/rebalancer/rebalancer.html
                    </aside>
                </section>
                <section>
                    <h2>Replication</h2>   
                    <p>
                        Fail-Over.
                        Maintain list of In Sync Replicas.
                    </p>
                    <p>
                        Brokers with replica's only keep these in sync with the Partition Master.
                    </p>
                    <aside class="notes">
                    Data from producers is first saved to a commit log before consumers can find out that it is
available. It will only be visible to consumers when the followers acknowledge that they have
got the data and stored in their local logs.
                    </aside>
                </section>
                
                <section>
                    <h2>Replication: They all die.</h2>   
                    <p>
                        Wait for an ISR replica to come back
                    </p>
                    <p>
                        Or choose the first replica which comes back.
                    </p>
                    <aside class="notes">
                        Tradeoff between availability and consistency.
                        
                        By default, Kafka chooses the second strategy and favor choosing a potentially inconsistent replica when all replicas in the ISR are dead. This behavior can be disabled using configuration property unclean.leader.election.enable, to support use cases where downtime is preferable to inconsistency.
                    </aside>
                </section>
                <section>
                    <h2>Replication: Acknowledgement</h2>  
                    <p>
                        Choose for acknowledgement by 0, 1 or all (ISR) Replicas
                    </p>
                    <p>
                        Extra options:
                        <ul>
                            <li>Disable unclean leader election.</li>
                            <li>Minimum ISR size.</li>
                        </ul>
                    
                    </p>
                </section>
                <section>
                    <h2>Producers</h2>  
                    <p>
                        Push messages into Kafka Topics.
                        <br/>
                        Can provide partition key.
                        Message gets forwarded to the leader.
                    </p>
                </section>
                <section>
                    <h2>Consumers</h2>   
                    <p>
                    Reads messages from a specific offset.
                    <br/>
                    Asks for messages - pull, as not to overload.     
                    </p>
                </section>
                <section>
                    <h2>Consumer Offset</h2>    
                </section>
                <section>
                    <h2>Consumer Groups</h2>    
                </section>
            </section>
            <section>
                <section>
                    <h2>When to Use It</h2>
                    <p>
                        <ul>
                            <li>Compared with RabbitMQ</li>
                            <li>Customer Use Cases</li>
                        </ul>
                    </p>
                </section>
                <section>
                    <h2>Vs RabbitMQ</h2>    
                </section>
                <section>
                    <h2>Use Case: Linkedin</h2>    
                </section>
                <section>
                    <h2>Use Case: New York Times</h2>    
                </section>
                <section>
                    <h2>Use Case: Zalando</h2>    
                </section>
                
            </section>
        
            <section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <img width="50%" src="img/kafka-apis.png">
                </section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <ul>
                        <li>Producer API: publish to a topic</li>
                        <li>Consumer API: subscribe to a topic</li>
                        <li>Streams API: stream processor</li>
                        <li>Connector API: resuable producers and consumers</li>
                    </ul>
                    <aside class="notes">
                        <lu>
                            <li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
                            <li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
                            <li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
                            <li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
                        </lu>
                    </aside>
                </section>
                
                
                <section>
                    <h2>Kafka: Kafka Streams</h2>
                    <img width="50%" src="img/streams-welcome.png">
                    <aside class="notes">
                        Simple and lightweight client library
                        No external dependencies - only to Kafka
                        Exactly-once is supported
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Kafka Streams</h2>
                    <img width="50%" src="img/streams-architecture-topology.jpg">
                    <aside class="notes">
                        A Stream represents an unbounded, continuously updating data set.
                        An ordered, replayable and fault-tolerant sequence of immutable data records
                        Stream processing application uses one or more processor topologies.
                        Stream processor is a note in the processor topology
                        Source Processor: produces and input stream
                        Sink Processor: has no down-stream processors
                    </aside>
                </section>
    
    
            </section>
        
        
            <section>
                <section>
                    <h2>Kafka: The Consumer Group</h2>
                    <p><ul>
                        <li>Divide Processing - like with a queue</li>
                        <li>Broadcast to multiple consumer groups - like publish-subscribe</li>
                        <li>Stronger Ordering Guarantees</li>
                        <li>Only 1 consumer per consumer group can process a partition</li>
                    </ul>
                        
                    </p>
                    
                    <aside class="notes">
                        Paralel processing over members of a consumer group.
                        Broadcast to other consumer groups
                        
                        This for every topic
                        
                        Strong ordering guarantees then traditional systems - queues provide records as received.
                        But if more consumers process then ordering can be lost.
                        Thanks to the partitions this ordering can be retained
                    </aside>
                </section>

            </section>
    
            <section>
                <section>
                    <h2>Kafka: Topics and Logs</h2>
                    <img width="50%" src="img/log_anatomy.png">
                    <aside class="notes">
                        A topic is to where records are published - always multi-subscriber (0 - n consumers)
                        
                        For each topic Kafka maintains a partitioned log
                        
                        Each partition is an ordered, immutable sequence of records that is continuously appended to.
                        Each record in a partition is assigned an offset, which uniquely identifies each record within a partition.
                        
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Topics and Logs</h2>
                    <ul>
                        <li>The cluster retains all published records</li>
                        <li>The consumer controls its offset</li>
                        <li>Low overhead per consumer</li>
                    </ul>
                    <aside class="notes">
                        All records are retained for a configurable time.
                        Only overhead per consumer is the current offset of that consumer.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Partitions</h2>
                    <ul>
                        <li>Allow the system to scale.</li>
                        <li>Are replicated</li>
                        <li>Each partition has 1 leader</li>
                        <li>Guarantees order of records</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        Kafka only provides a total order over records within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Producers</h2>
                    <ul>
                        <li>Will assign record to a partition</li>
                        <li>Round Robin or based on a key</li>
                    </ul>
                    <aside class="notes">

                    </aside>
                </section>
                <section>
                    <h2>Kafka: Consumers</h2>
                    <ul>
                        <li>Consumers are part of a Consumer Group</li>
                        <li>1 record published to 1 consumer instance</li>
                        <li>Partitions will determine max. # of consumers</li>
                        <li>Scalable within a Consumer Group</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a "fair share" of partitions at any point in time. This process of maintaining membership in the group is handled by the Kafka protocol dynamically. If new instances join the group they will take over some partitions from other members of the group; if an instance dies, its partitions will be distributed to the remaining instances.
                    </aside>
                </section>
    
    
    
            </section>

    
            

            <section>
                <section>
                    <h2>Hands On</h2>
                
                </section>
                
                <section>
                    <h2>Hands On: Agenda</h2>
                    <p>
                        <ul>
                            <li>Prerequisites</li>
                            <li>Docker</li>
                            <li>RabbitMQ</li>
                            <li>The application</li>
                            <li>DIY: Native</li>
                            <li>DIY: Reactor</li>
                            <li>DIY: Spring Cloud Stream</li>
                            <li>DIY: Use RabbitMQ</li>
                        </ul>
                    </p>
                </section>
                <section>
                    <h2>Pre-Requisites</h2>
                </section>
                
                
                
                
                
                
                
                
                
                
                
                
                
                <section>
                    <h2>Docker</h2>
                    <ul>
                        <li>Build: standardized way using Docker CLI in a DockerFile</li>
                        <li>Ship: share the container - it is just a file</li>
                        <li>Run: less overhead then a VM</li>
                    </ul>
                    <aside class="notes">
                        Docker runs on a minimal operation system so it has much less overhead then a traditional VM.
                    </aside>
                </section>
                <section>
                    <h2>Docker Compose</h2>
                    <ul>
                        <li>Multi-container applications</li>
                        <li>docker-compose.yml</li>
                        <li>Software defined network</li>
                    </ul>
                    <aside class="notes">
                        Defined in a docker-compose file: great tool for setting up development, staging and CI environments.
                        
                        Docker-compose helps you by linking these containers.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: DockerFile</h2>
                    <img width="100%" src="img/single/download-smaller.png">
                    <aside class="notes">
                        Based on the Spotify Kafka Docker image.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Create the image</p>
                     <pre><code data-trim contenteditable>
                     docker build -t kafka-zookeeper .
                     </code></pre>
                    <p>Startup the container</p>
                    <pre><code data-trim contenteditable>
                     docker run --name a_name -i -t -p 2181:2181 -p 9092:9092 kafka-zookeeper
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Access Kafka from outside the container: add the following parameters</p>
                     <pre><code data-trim contenteditable>
                     -e ADVERTISED_HOST=localhost -e ADVERTISED_PORT=9092
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Topics</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --create --zookeeper localhost:2181 
                    --replication-factor 1 --partitions 10 --topic test
                     </code></pre>
                    <p>Retrieve a list of topics on Zookeeper</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --list --zookeeper localhost:2181
                     </code></pre>
                    <p>Describe a topic: leader / replicated nodes</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --describe --zookeeper localhost:2181
                     </code></pre>
                    <aside class="notes">
                        You can have as much partitions as you want - but for replication you will need more then 1 node.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: Publish and Subscribe</h2>
                    <p>Producer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
                     </code></pre>
                    <p>Single Container: Consumer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
                    --topic test --from-beginning
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Result</h2>
                    <ul>
                        <li>Single Container</li>
                        <li>1 zookeeper and 1 kafka node</li>
                        <li>Data not stored</li>
                    </ul>
                </section>
                
                <section>
                    <h2>Demo: Do It Yourself</h2>
                    <p>https://github.com/tomvdbulck/kafka-docker</p>
                </section>
            </section>

            

    
    

            <section>
                <section>
                    <h2>Questions ?</h2>

                </section>
            </section>




            <!-- **********
                DO NOT REMOVE
            ********** -->
            <section style="text-align: left;">
                <h2>Thanks for watching!</h2>
                <p class="fragment">Now kick some ass!</p>
            </section>

            <section style="text-align: left;" data-background="img/jworks-wallpaper-3.jpg"></section>
        </div>
    </div>
    <script src="js/reveal.js"></script>
    <script>
        Reveal.initialize({
            transition: 'convex',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                {
                    src: 'lib/js/classList.js',
                    condition: function () {
                        return !document.body.classList;
                    }
                },
                // Interpret Markdown in <section> elements
                {
                    src: 'plugin/markdown/marked.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                {
                    src: 'plugin/markdown/markdown.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                // Syntax highlight for <code> elements
                {
                    src: 'plugin/highlight/highlight.js',
                    async: true,
                    callback: function () {
                        hljs.initHighlightingOnLoad();
                    }
                },
                // Zoom in and out with Alt+click
                {
                    src: 'plugin/zoom-js/zoom.js',
                    async: true
                },
                // Speaker notes
                {
                    src: 'plugin/notes/notes.js',
                    async: true
                }
        ]
        });
    </script>
</body>

</html>

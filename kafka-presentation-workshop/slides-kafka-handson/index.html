<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <title>JWorks - Kafka DIY</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/ordina.css" id="theme">
    <link rel="stylesheet" href="css/custom.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <script src="js/head.min.js"></script>
    <!--Add support for earlier versions of Internet Explorer -->
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Kafka</h1>
                <h3>Do It Yourself</h3>
                <img src="img/jworks-logo.png" style="vertical-align: bottom;" width="22.5%"/>
            </section>

            <section>
                <h2>Hi, my name is Tom.</h2>
                <p>
                    Developer<br />
                    Ordina Belgium<br />
                    @tomvdbulck<br />
                    https://github.com/tomvdbulck
                </p>
            </section>
            
            <section>
                <h2>Prerequisites</h2>
                <p>
                    https://github.com/tomvdbulck/kafka-handson-workshop
                </p>
                <p>
                    Docker<br/>
                    Postman<br/>
                    RabbitMQ<br/>
                </p>
            </section>

            <!-- **********
                EDIT FROM HERE
            ********** -->
            <section>
                <h2>Agenda</h2>
                <ul>
                    <li>What is Kafka</li>
                    <li>A Kafka Cluster</li>
                    <li>Kafka: When to Use it</li>
                    <li>Kafka: 4 core API's</li>
                    <li>Hands-On</li>
                </ul>
            </section>

            <!-- Example of nested vertical slides -->
            <section>
                <section>
                    <h2>Kafka</h2>

                    <img width="50%" src="img/kafka_logo.png">
                </section>
                <section>
                    <h2>Kafka</h2>
                    <img width="50%" src="img/kafka_diagram.png">
                    
                    <aside class="notes">
                    <ul>
                        <li>Messaging</li>
                        <li>Storage data - event log</li>
                        <li>Streams</li>
                    </ul></aside>
                </section>
                
                
                
                
                <section>
                    <h2>Kafka: Design Goals</h2>
                    <p> High volume publish-subscribe message and streams</p>
                    <p>
                        Durable<br/>
                        Fast<br/>
                        Scalable<br/>
            
                    </p>
                    <aside class="notes">
                        At its essence, Kafka provides a durable message store, similar to a log, run in a server cluster, that stores streams of records in categories called topics
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Dumb Brokers</h2>
                    <p>
                        Smart Consumers <br/>
                        Retains all messages <br/>
                        
                    </p>
                    <aside class="notes"></aside>
                </section>
            </section>
            <section>
                <section>
                    <h2>A Kafka Cluster</h2>
                <p>
                    <ul>
                        <li>Zookeeper</li>
                        <li>Broker</li>
                        <li>Controller</li>
                        <li>Topics</li>
                        <li>Partitions</li>
                        <li>Replication</li>
                        <li>Producers</li>
                        <li>Consumers</li>
                        <li>Consumer offset</li>
                        <li>Failover</li>
                    </ul>
                </p>
                
                </section>
                
        
                <section>
                    <h2>Zookeeper</h2>

                    <img width="20%" src="img/zookeeper_logo.jpeg">
                    
                    <aside class="notes">
                        A Distributed Coordination Service
                        For distributed Applications
                        Data model styled after the familiar tree structure of file systems
                        Easy to program to so that distributed applications can build upon for synchronization, configuration maintenance and groups and naming.
                    </aside>   
                </section>
                <section>
                    <h2>Zookeeper: Why does Kafka Need it</h2>
                    <p>
                        <ul>
                            <li>Electing a Controller</li>
                            <li>Cluster Membership</li>
                            <li>Topic Configuration</li>
                            <li>Quotas</li>
                            <li>ACL: Who is allowed to read and write</li>
                        </ul>
                    
                    </p>
                <aside class="notes">
                        Cluster membership: which brokers are alive and part of the cluster.
                    
                        Which topics exist, how many partitions, where are the replicas, ... 
                        
                        How much data is each client allowed to read and write
                        
                        Kafka comes with Kafka Authorization Command Line - define uses and access
                    </aside>
                </section>
                <section>
                    <h2>Brokers</h2>   
                    <p>
                        Node, Broker, Kafka Server => all the same
                    </p>
                    <p>
                        Hosts topics<br/>
                        Stores messages
                    </p>
                    <aside class="notes">
                        Stores messages on disk by a unique offset
                        Shares information between other brokers and zookeeper
                    </aside>
                </section>
                <section>
                    <h2>The Controller</h2>  
                    <p>Manage State of Partitions and Replicas</p>
                    <p>Reassign Partitions</p>
            
                    <aside class="notes">
                         The controler is one of the brokers, a broker is a kafka node, which is is responsible to maintain the leader/follower relationship for all Partitions. Zookeepers makes sure there is 1 and if it crashed will elect a new one. The controller will tell replicas to become partition leaders - can reshuffle partitions, ... 
                        
                        see: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Controller+Internals
                    </aside>
                </section>
                <section>
                    <h2>Topics</h2>  
                    <p>
                        Virtual Group of partitions
                    </p>
                    <p>
                        Producers write messages.<br/>
                        Consumes read messages.
                    </p>
                </section>
                <section>
                    <h2>Partitions</h2>    
                    <p>
                        Topics are split in partitions.<br/>
                        1 broker is leader.
                    </p>
                    <p>
                        Used to scale.
                    </p>
                    <aside class="notes">
                        Producers and Consumer only speak with the leader.
                        
                        A list of insync replica's is maintained by the brokers / partition.
                        New leader is elected if required.
                        
                        Used to scale up or down the cluster.
                        As consumers and producers approach the cluster via partitions.
                    </aside>
                </section>
                <section>
                    <h2>Partitions: Rebalancing</h2>    
                    <p> auto.leader.rebalance.enable: true (default) </p>
                        
                    <p>    also manual rebalancing possible
                    </p>
                    <aside class="notes">
                    
                        kafka-preferred-replica-election.sh tool
                        
                        see: https://docs.confluent.io/current/kafka/rebalancer/rebalancer.html
                    </aside>
                </section>
                <section>
                    <h2>Replication</h2>   
                    <p>
                        Fail-Over.<br/>
                        Each broker: Maintains list of In Sync Replicas.
                    </p>
                    <p>
                        Brokers with replicas only keep these in sync with the Partition Master.
                    </p>
                    <aside class="notes">
                    Data from producers is first saved to a commit log before consumers can find out that it is
available. It will only be visible to consumers when the followers acknowledge that they have
got the data and stored in their local logs.
                    </aside>
                </section>
                
                <section>
                    <h2>Replication: They all die.</h2>   
                    <p>
                        Wait for an ISR replica to come back
                    </p>
                    <p>
                        Or<br/> 
                        choose the first replica which comes back.
                    </p>
                    <aside class="notes">
                        Tradeoff between availability and consistency.
                        
                        By default, Kafka chooses the second strategy and favor choosing a potentially inconsistent replica when all replicas in the ISR are dead. This behavior can be disabled using configuration property unclean.leader.election.enable, to support use cases where downtime is preferable to inconsistency.
                    </aside>
                </section>
                <section>
                    <h2>Replication: Acknowledgement</h2>  
                    <p>
                        Choose for acknowledgement by 0, 1 or all (ISR) Replicas.<br/> 
                        Only for producers.
                    </p>
                    <p>
                        Extra options:<br/> 
                        
                        Disable unclean leader election.<br/> 
                        Minimum ISR size.<br/> 
                    
                    </p>
                </section>
                <section>
                    <h2>Producers</h2>  
                    <p>
                        Push messages into Kafka Topics.<br/> 
                        <br/>
                        Can provide partition key.<br/> 
                        Message gets forwarded to the leader.
                    </p>
                </section>
                <section>
                    <h2>Messages / Records</h2>
                    <p>
                        Unique offset within partition.<br/>
                        Remain until TTL or after compaction.
                    </p>
                    <p>
                        <ul>
                        <li>key</li>
                        <li>value</li>
                        <li>timestamp</li>
                        </ul>
                    </p>
                    <aside class="notes">
                        Partitioning occurs on this key.
                        
                        Please note that offset <> key.
                    </aside>
                </section>    
                <section>
                    <h2>Consumers</h2>   
                    <p>
                    Reads messages from a specific offset.
                    <br/>
                    Asks for messages - pull, as not to overload.     
                    </p>
                    <br/>
                    Can only read messages which are fully in sync.
                </section>
                <section>
                    <h2>Consumer Offset</h2> 
                    <p>
                        Stored in Kafka. (previously Zookeeper)
                        <br/>
                        Exactly once: read and commit offset in transaction
                    </p>
                    <p>
                        The consumer can change this offset.
                    </p>
                </section>
                <section>
                    <h2>Consumer Groups</h2> 
                    <p>
                        1 partition can be read by 1 consumer of the group.
                        </br>
                        parallel processing.
                    </p>
                </section>
            </section>
            <section>
                <section>
                    <h2>When to Use It</h2>
                    <p>
                        <ul>
                            <li>Compared with RabbitMQ</li>
                            <li>Customer Use Cases</li>
                        </ul>
                    </p>
                </section>
                <section>
                    <h2>Vs RabbitMQ</h2>    

                    <aside class="notes">
                    See: https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka
                    </aside>
                </section>
                

                <section>
                    <h2>Vs RabbitMQ: choose (Rabbit)MQ when: </h2>    
                    <p>
                        More control/guarantees per-message.<br/> 
                        Complex routing to consumers.<br/> 
                        More variety in: point to point, request/reply and publish/subscribe messaging<br/> 
                    </p>
                    <aside class="notes">
                    
                    </aside>
                </section>
                <section>
                    <h2>Vs RabbitMQ: Choose Kafka when: </h2>    
                    <p>
                        Stream with complex routing.<br/> 
                        Throughput (100k/sec)<br/>   
                        Event Sourcing<br/> 
                        
                    </p>
                    <aside class="notes">
                    
                    </aside>
                </section>
                <section>
                    <h2>Vs RabbitMQ: choose (Rabbit)MQ when: </h2>    
                    <p>
                        
                        Ease of Operations.<br/> 
                        Security.<br/>  
                       
                    </p>
                    <aside class="notes">
                    It is a more mature eco system.
                    </aside>
                </section>
                <section>
                    <h2>Vs RabbitMQ: choose Kafka when: </h2>    
                    <p>
                        Performance.<br/> 
                        Can deal with large surge of Producers.<br/> 
                    </p>
                    <aside class="notes">
                    
                    </aside>
                </section>
                <section>
                    <h2>Use Cases</h2>
                    <p>
                        LinkedIn </br>
                        New York Times </br>
                        Zalando
                    </p>
                </section>
                <section>
                    <h2>Use Case: Linkedin</h2>    
                    <img width="50%" src="img/apache-kafka-at-linkedin-26-638.jpg">
                    <p>
                        Used to move every type of data between other systems.
                    </p>
                    <aside class="notes">
                        See:
                            https://engineering.linkedin.com/kafka/running-kafka-scale
                            https://engineering.linkedin.com/blog/2017/08/open-sourcing-kafka-cruise-control
                    </aside>
                </section>
                <section>
                    <h2>Use Case: Linkedin</h2>    
                    <p>
                        2 trillion messages / day <br/>
                        1800 brokers<br/> 
                        650 terabytes consumed for 170 terrabytes of messages (in 2015 - 800 billion msg/day) <br/> 
                    </p>
                </section>
                <section>
                    <h2>Use Case: New York Times</h2> 
                    <img width="50%" src="img/nyt-kafka--old-arch.png">
                    <p>
                        API based architecture
                    </p>
                    <aside class="notes">
                        See: https://www.confluent.io/blog/publishing-apache-kafka-new-york-times
                        The Monolog is our new source of truth for published content. Every system that creates content, when it’s ready to be published, will write it to the Monolog, where it is appended to the end. The actual write happens through a gateway service, which validates that the published asset is compliant with our schema.
                    </aside>   
                </section>

                <section>
                    <h2>Use Case: New York Times</h2> 
                    <img width="50%" src="img/nyt-kafka-arch.png">
                    <aside class="notes">
                    </aside>   
                </section>
                <section>
                    <h2>Use Case: New York Times</h2> 
                    <img width="50%" src="img/nyt-publish-to-es.png">
                    <aside class="notes">
                    </aside>   
                </section>
                <section>
                    <h2>Use Case: New York Times</h2> 
                    <p>
                        Simplify development process.<br/>
                        Tracking of every individual update throughout the system. <br/>
                        Mental shift for Developers required.
                    </p>
                    <aside class="notes">
                         The fact that all content is coming through the same pipeline is simplifying our software development processes, both for front-end applications and back-end systems. 
                        
                        Finally, this is a new way of building applications, and it requires a mental shift for developers who are used to working with databases and traditional pub/sub-models. In order to take full advantage of this setup, we need to build applications in such a way that it is easy to deploy new instances that use replay to recreate their materialized view of the log, and we are putting a lot of effort into providing tools and infrastructure that makes this easy.


                    </aside>   
                </section>
                <section>
                    <h2>Use Case: Zalando</h2>       
                    <p>
                        New platform:<br/> throughput/capacity<br/> 
                        democratization of data<br/> 
                        event-driven system: event source processing<br/> 
                    
                    </p>
                    <aside class="notes">
                        See: https://jobs.zalando.com/tech/blog/event-first-development---moving-towards-kafka-pipeline-applications/
                        First choses as the outbound event platform ... just natural to use if for inbound as well => event source processing made easy.
                    </aside> 
                </section>
                
            </section>
        
            <section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <img width="50%" src="img/kafka-apis.png">
                </section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <ul>
                        <li>Producer API: publish to a topic</li>
                        <li>Consumer API: subscribe to a topic</li>
                        <li>Streams API: stream processor</li>
                        <li>Connector API: resuable producers and consumers</li>
                    </ul>
                    <aside class="notes">
                        <lu>
                            <li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
                            <li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
                            <li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
                            <li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
                        </lu>
                    </aside>
                </section>
                
                <section>
                    <h2>Producer API</h2>
                    
                
                </section>
                
                <section>
                    <h2>Consumer API</h2>
                    
                
                </section>
                
                
                <section>
                    <h2>Kafka Streams</h2>
                    <img width="50%" src="img/streams-welcome.png">
                    <aside class="notes">
                        Simple and lightweight client library
                        No external dependencies - only to Kafka
                        Exactly-once is supported
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Kafka Streams</h2>
                    <img width="50%" src="img/streams-architecture-topology.jpg">
                    <aside class="notes">
                        A Stream represents an unbounded, continuously updating data set.
                        An ordered, replayable and fault-tolerant sequence of immutable data records
                        Stream processing application uses one or more processor topologies.
                        Stream processor is a note in the processor topology
                        Source Processor: produces and input stream
                        Sink Processor: has no down-stream processors
                    </aside>
                </section>
                
                <section>
                    <h2>Connectors</h2>
                    
                
                </section>
    
    
            </section>
        

            <section>
                <section>
                    <h2>Hands On</h2>
                
                </section>
                
                <section>
                    <h2>Hands On: Agenda</h2>
                    <p>
                        <ul>
                            <li>Prerequisites</li>
                            <li>Docker</li>
                            <li>RabbitMQ</li>
                            <li>The application</li>
                            <li>DIY: Native</li>
                            <li>DIY: Reactor</li>
                            <li>DIY: Spring Cloud Stream</li>
                            <li>DIY: Use RabbitMQ</li>
                        </ul>
                    </p>
                </section>
                <section>
                    <h2>Pre-Requisites</h2>
                    <aside class="notes"></aside>
                </section>
                <section>
                    <img width="50%" src="img/docker-horizontal.png">
                    <aside class="notes"></aside>
                </section>
                <section>
                    <img width="50%" src="img/RabbitMQ-logo.svg">
                    <p>
                        https://www.rabbitmq.com/download.html
                    </p>
                    <p>
                        <img width="50%" src="img/rabbitmq-config.png">
                    </p>
                    <p>
                        Go to http://localhost:15672/ <br/>
                        Use: guest/guest (super safe)
                    </p>
                    <p>
                        Verify if the port config in application.yml corresponds.
                    </p>
                    <aside class="notes">
                        https://www.rabbitmq.com/download.html
                        
                        > brew install rabbitmq
                        
                        > cd /usr/local/sbin
                        
                        > ./rabbitmq-server
                        
                        Go to http://localhost:15672/
                        
                        User/Pwd: guest / guest
                    
                    </aside>
                </section>
                <section>
                    <h2>Docker</h2>
                    <aside class="notes"></aside>
                </section>
                <section>
                    <h2>DIY: Native</h2>
                    <aside class="notes"></aside>
                </section>
                <section>
                    <h2>DIY: Reactor</h2>
                    <aside class="notes"></aside>
                </section>
                <section>
                    <h2>DIY: Spring Cloud Stream</h2>
                    <aside class="notes"></aside>
                </section>
                <section>
                    <h2>DIY: switch to RabbitMQ</h2>
                    <img width="50%" src="img/set-rabbitmq.png">
                    <p>set defaultCandidate of rabbit to true</p>
                </section>

                
                
                
                
                
                
                
                
                
                
                
                
                
                <section>
                    <h2>Docker</h2>
                    <ul>
                        <li>Build: standardized way using Docker CLI in a DockerFile</li>
                        <li>Ship: share the container - it is just a file</li>
                        <li>Run: less overhead then a VM</li>
                    </ul>
                    <aside class="notes">
                        Docker runs on a minimal operation system so it has much less overhead then a traditional VM.
                    </aside>
                </section>
                <section>
                    <h2>Docker Compose</h2>
                    <ul>
                        <li>Multi-container applications</li>
                        <li>docker-compose.yml</li>
                        <li>Software defined network</li>
                    </ul>
                    <aside class="notes">
                        Defined in a docker-compose file: great tool for setting up development, staging and CI environments.
                        
                        Docker-compose helps you by linking these containers.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: DockerFile</h2>
                    <img width="100%" src="img/single/download-smaller.png">
                    <aside class="notes">
                        Based on the Spotify Kafka Docker image.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Create the image</p>
                     <pre><code data-trim contenteditable>
                     docker build -t kafka-zookeeper .
                     </code></pre>
                    <p>Startup the container</p>
                    <pre><code data-trim contenteditable>
                     docker run --name a_name -i -t -p 2181:2181 -p 9092:9092 kafka-zookeeper
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Access Kafka from outside the container: add the following parameters</p>
                     <pre><code data-trim contenteditable>
                     -e ADVERTISED_HOST=localhost -e ADVERTISED_PORT=9092
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Topics</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --create --zookeeper localhost:2181 
                    --replication-factor 1 --partitions 10 --topic test
                     </code></pre>
                    <p>Retrieve a list of topics on Zookeeper</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --list --zookeeper localhost:2181
                     </code></pre>
                    <p>Describe a topic: leader / replicated nodes</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --describe --zookeeper localhost:2181
                     </code></pre>
                    <aside class="notes">
                        You can have as much partitions as you want - but for replication you will need more then 1 node.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: Publish and Subscribe</h2>
                    <p>Producer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
                     </code></pre>
                    <p>Single Container: Consumer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
                    --topic test --from-beginning
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Result</h2>
                    <ul>
                        <li>Single Container</li>
                        <li>1 zookeeper and 1 kafka node</li>
                        <li>Data not stored</li>
                    </ul>
                </section>
                
                <section>
                    <h2>Demo: Do It Yourself</h2>
                    <p>https://github.com/tomvdbulck/kafka-docker</p>
                </section>
            </section>

            

    
    

            <section>
                <section>
                    <h2>Questions ?</h2>

                </section>
            </section>




            <!-- **********
                DO NOT REMOVE
            ********** -->
            <section style="text-align: left;">
                <h2>Thanks for watching!</h2>
                <p class="fragment">Now kick some ass!</p>
            </section>

            <section style="text-align: left;" data-background="img/jworks-wallpaper-3.jpg"></section>
        </div>
    </div>
    <script src="js/reveal.js"></script>
    <script>
        Reveal.initialize({
            transition: 'convex',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                {
                    src: 'lib/js/classList.js',
                    condition: function () {
                        return !document.body.classList;
                    }
                },
                // Interpret Markdown in <section> elements
                {
                    src: 'plugin/markdown/marked.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                {
                    src: 'plugin/markdown/markdown.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                // Syntax highlight for <code> elements
                {
                    src: 'plugin/highlight/highlight.js',
                    async: true,
                    callback: function () {
                        hljs.initHighlightingOnLoad();
                    }
                },
                // Zoom in and out with Alt+click
                {
                    src: 'plugin/zoom-js/zoom.js',
                    async: true
                },
                // Speaker notes
                {
                    src: 'plugin/notes/notes.js',
                    async: true
                }
        ]
        });
    </script>
</body>

</html>

<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <title>JWorks - Kafka on Docker</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/ordina.css" id="theme">
    <link rel="stylesheet" href="css/custom.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <script src="js/head.min.js"></script>
    <!--Add support for earlier versions of Internet Explorer -->
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <h1>Kafka on Docker</h1>
                <h3>Containerize it all</h3>
                <img src="img/jworks-logo.png" style="vertical-align: bottom;" width="22.5%"/>
            </section>

            <section>
                <h2>Hi, my name is Tom.</h2>
                <p>
                    Developer<br />
                    Ordina Belgium<br />
                    @tomvdbulck<br />
                    https://github.com/tomvdbulck
                </p>
            </section>

            <!-- **********
                EDIT FROM HERE
            ********** -->
            <section>
                <h2>Agenda</h2>
                <ul>
                    <li>Kafka</li>
                    <li>Zookeeper</li>
                    <li>Docker</li>
                    <li>All in One</li>
                    <li>Scale Up</li>
                    <li>Kafka Monitor</li>
                </ul>
            </section>

            <!-- Example of nested vertical slides -->
            <section>
                <section>
                    <h2>Kafka</h2>

                    <img width="50%" src="img/kafka_logo.png">
                </section>
                <section>
                    <h2>Kafka</h2>
                    <img width="50%" src="img/kafka_diagram.png">
                    <aside class="notes"><ul>
                        <li>Publish and Subscribe</li>
                        <li>Process Streams of Data</li>
                        <li>Store</li>
                    </ul></aside>
                </section>
                <section>
                    <h2>Kafka: Topics and Logs</h2>
                    <img width="50%" src="img/log_anatomy.png">
                    <aside class="notes">
                        A topic is to where records are published - always multi-subscriber (0 - n consumers)
                        
                        For each topic Kafka maintains a partitioned log
                        
                        Each partition is an ordered, immutable sequence of records that is continuously appended to.
                        Each record in a partition is assigned an offset, which uniquely identifies each record within a partition.
                        
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Topics and Logs</h2>
                    <ul>
                        <li>The cluster retains all published records</li>
                        <li>The consumer controls its offset</li>
                        <li>Low overhead per consumer</li>
                    </ul>
                    <aside class="notes">
                        All records are retained for a configurable time.
                        Only overhead per consumer is the current offset of that consumer.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Partitions</h2>
                    <ul>
                        <li>Allow the system to scale.</li>
                        <li>Are replicated</li>
                        <li>Each partition has 1 leader</li>
                        <li>Guarantees order of records</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        Kafka only provides a total order over records within a partition, not between different partitions in a topic. Per-partition ordering combined with the ability to partition data by key is sufficient for most applications.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Producers</h2>
                    <ul>
                        <li>Will assign record to a partition</li>
                        <li>Round Robin or based on a key</li>
                    </ul>
                    <aside class="notes">

                    </aside>
                </section>
                <section>
                    <h2>Kafka: Consumers</h2>
                    <ul>
                        <li>Consumers are part of a Consumer Group</li>
                        <li>1 record published to 1 consumer instance</li>
                        <li>Partitions will determine max. # of consumers</li>
                        <li>Scalable within a Consumer Group</li>
                    </ul>
                    <aside class="notes">
                        The contents of a partition must fit on 1 server
                        They are replicated for fault tolerance
                        Each leader handles all read and write requests, while the followers passively replicate.
                        Each server acts as a leader for some partitions and follower for others so the load gets distributed.
                        The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a "fair share" of partitions at any point in time. This process of maintaining membership in the group is handled by the Kafka protocol dynamically. If new instances join the group they will take over some partitions from other members of the group; if an instance dies, its partitions will be distributed to the remaining instances.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <img width="50%" src="img/kafka-apis.png">
                </section>
                <section>
                    <h2>Kafka: 4 Core API's</h2>
                    <ul>
                        <li>Producer API: publish to a topic</li>
                        <li>Consumer API: subscribe to a topic</li>
                        <li>Streams API: stream processor</li>
                        <li>Connector API: resuable producers and consumers</li>
                    </ul>
                    <aside class="notes">
                        <lu>
                            <li>The Producer API allows an application to publish a stream of records to one or more Kafka topics.</li>
                            <li>The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.</li>
                            <li>The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li>
                            <li>The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li>
                        </lu>
                    </aside>
                </section>
                
                
                <section>
                    <h2>Kafka: Kafka Streams</h2>
                    <img width="50%" src="img/streams-welcome.png">
                    <aside class="notes">
                        Simple and lightweight client library
                        No external dependencies - only to Kafka
                        Exactly-once is supported
                    </aside>
                </section>
                <section>
                    <h2>Kafka: Kafka Streams</h2>
                    <img width="50%" src="img/streams-architecture-topology.jpg">
                    <aside class="notes">
                        A Stream represents an unbounded, continuously updating data set.
                        An ordered, replayable and fault-tolerant sequence of immutable data records
                        Stream processing application uses one or more processor topologies.
                        Stream processor is a note in the processor topology
                        Source Processor: produces and input stream
                        Sink Processor: has no down-stream processors
                    </aside>
                </section>
                <section>
                    <h2>Kafka: A Storage System</h2>
                    <ul>
                        <li>Kafka is a very good storage system</li>
                        <li>Written to Disk and Replicated</li>
                        <li>No Performance Loss</li>
                        <li>https://www.confluent.io/blog/okay-store-data-apache-kafka/</li>
                    </ul>
                    <aside class="notes">
                        Any message queue that allows publishing messages decoupled from consuming them is effectively acting as a storage system for the in-flight messages. What is different about Kafka is that it is a very good storage system.
                        Data written to kafka is written to disk and replicated.
                        Allows producers to wait on acknowledgement so that a write is only considered complete when replicated and guaranteed to persist event if the server fails
                        
                        The disk structures Kafka uses scale well—Kafka will perform the same whether you have 50 KB or 50 TB of persistent data on the server.

As a result of taking storage seriously and allowing the clients to control their read position, you can think of Kafka as a kind of special purpose distributed filesystem dedicated to high-performance, low-latency commit log storage, replication, and propagation.



                    </aside>
                </section>
                <section>
                    <h2>Kafka: A Messaging System</h2>
                    <p> Traditionally 2 models
                    <ul>
                        <li>Queueing: process record once</li>
                        <li>Publish and Subscribe: broadcast</li>
                        
                    </ul>
                    </p>
                    <aside class="notes">
                        Messaging traditionally has two models: queuing and publish-subscribe. In a queue, a pool of consumers may read from a server and each record goes to one of them; in publish-subscribe the record is broadcast to all consumers. Each of these two models has a strength and a weakness. The strength of queuing is that it allows you to divide up the processing of data over multiple consumer instances, which lets you scale your processing. Unfortunately, queues aren't multi-subscriber—once one process reads the data it's gone. Publish-subscribe allows you broadcast data to multiple processes, but has no way of scaling processing since every message goes to every subscriber.
                    </aside>
                </section>
                <section>
                    <h2>Kafka: The Consumer Group</h2>
                    <p><ul>
                        <li>Divide Processing - like with a queue</li>
                        <li>Broadcast to multiple consumer groups - like publish-subscribe</li>
                        <li>Stronger Ordering Guarantees</li>
                        <li>Only 1 consumer per consumer group can process a partition</li>
                    </ul>
                        
                    </p>
                    
                    <aside class="notes">
                        Paralel processing over members of a consumer group.
                        Broadcast to other consumer groups
                        
                        This for every topic
                        
                        Strong ordering guarantees then traditional systems - queues provide records as received.
                        But if more consumers process then ordering can be lost.
                        Thanks to the partitions this ordering can be retained
                    </aside>
                </section>

            </section>

            <section>
                <section>
                    <h2>Zookeeper</h2>

                    <img width="20%" src="img/zookeeper_logo.jpeg">
                </section>
                <section>
                    <h2>Zookeeper</h2>

                    <img width="70%" src="img/zookeeper%20service.png">
                    <aside class="notes">
                        A Distributed Coordination Service
                        For distributed Applications
                        Data model styled after the familiar tree structure of file systems
                        Easy to program to so that distributed applications can build upon for synchronization, configuration maintenance and groups and naming.
                    </aside>    
                </section>
                <section>
                    <h2>Zookeeper</h2>
                    <ul>
                        <li>Distributed Coordination Service</li>
                        <li>For Distributed Applications</li>
                        <li>File System Like Structure</li>
                    </ul>
                </section>
                <section>
                    <h2>Zookeeper: Why does Kafka Need it</h2>
                    <p>
                        <ul>
                            <li>Electing a Controller</li>
                            <li>Cluster Membership</li>
                            <li>Topic Configuration</li>
                            <li>Quotas</li>
                            <li>ACL: Who is allowed to read and write</li>
                        </ul>
                    
                    </p>
                <aside class="notes">
                        The controler is one of the brokers, a broker is a kafka node, which is is responsible to maintain the leader/follower relationship for all Partitions. Zookeepers makes sure there is 1 and if it crashed will elect a new one. The controller will tell replicas to become partition leaders - can reshuffle partitions, ... 
                    
                        Cluster membership: which brokers are alive and part of the cluster.
                    
                        Which topics exist, how many partitions, where are the replicas, ... 
                        
                        How much data is each client allowed to read and write
                        
                        Kafka comes with Kafka Authorization Command Line - define uses and access
                    </aside>
                </section>
            </section>

            <section>
                <section>
                    <h2>Docker</h2>

                    <img width="50%" src="img/docker-horizontal.png">
                </section>
                <section>
                    <h2>Docker</h2>
                    <ul>
                        <li>Build: standardized way using Docker CLI in a DockerFile</li>
                        <li>Ship: share the container - it is just a file</li>
                        <li>Run: less overhead then a VM</li>
                    </ul>
                    <aside class="notes">
                        Docker runs on a minimal operation system so it has much less overhead then a traditional VM.
                    </aside>
                </section>
                <section>
                    <h2>Docker Compose</h2>
                    <ul>
                        <li>Multi-container applications</li>
                        <li>docker-compose.yml</li>
                        <li>Software defined network</li>
                    </ul>
                    <aside class="notes">
                        Defined in a docker-compose file: great tool for setting up development, staging and CI environments.
                        
                        Docker-compose helps you by linking these containers.
                    </aside>
                </section>
            </section>

            <section>
                <section>
                    <h2>All in 1 Container</h2>

                    <img width="50%" src="img/lonely_container.JPG">
                </section>
                <section>
                    <h2>Single Container: The Goal</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                </section>
                <section>
                    <h2>Single Container: DockerFile</h2>
                    <img width="100%" src="img/single/download-smaller.png">
                    <aside class="notes">
                        Based on the Spotify Kafka Docker image.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: DockerFile</h2>
                    <img width="100%" src="img/single/get-scripts.png">
                </section>
                <section>
                    <h2>Single Container: Startup Scripts</h2>
                     <img width="100%" src="img/single/start-zookeeper.png">
                </section>
                <section>
                    <h2>Single Container: Startup Scripts</h2>
                     <img width="100%" src="img/single/start-kafka.png">
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Create the image</p>
                     <pre><code data-trim contenteditable>
                     docker build -t kafka-zookeeper .
                     </code></pre>
                    <p>Startup the container</p>
                    <pre><code data-trim contenteditable>
                     docker run --name a_name -i -t -p 2181:2181 -p 9092:9092 kafka-zookeeper
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Get Started</h2>
                    <p>Access Kafka from outside the container: add the following parameters</p>
                     <pre><code data-trim contenteditable>
                     -e ADVERTISED_HOST=localhost -e ADVERTISED_PORT=9092
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Topics</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --create --zookeeper localhost:2181 
                    --replication-factor 1 --partitions 10 --topic test
                     </code></pre>
                    <p>Retrieve a list of topics on Zookeeper</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --list --zookeeper localhost:2181
                     </code></pre>
                    <p>Describe a topic: leader / replicated nodes</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --describe --zookeeper localhost:2181
                     </code></pre>
                    <aside class="notes">
                        You can have as much partitions as you want - but for replication you will need more then 1 node.
                    </aside>
                </section>
                <section>
                    <h2>Single Container: Publish and Subscribe</h2>
                    <p>Producer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
                     </code></pre>
                    <p>Single Container: Consumer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 
                    --topic test --from-beginning
                     </code></pre>
                </section>
                <section>
                    <h2>Single Container: Result</h2>
                    <ul>
                        <li>Single Container</li>
                        <li>1 zookeeper and 1 kafka node</li>
                        <li>Data not stored</li>
                    </ul>
                </section>
            </section>

            <section>
                <section>
                    <h2>Scale Up: a Cluster</h2>

                    <img width="50%" src="img/blue-containers.jpg">
                </section>
                <section>
                    <h2>Cluster: The Goal</h2>
                    <p>A Zookeeper Cluster and a Dynamically Scalable Kafka Cluster</p>
                </section>
                <section>
                    <h2>Cluster: DockerFile Zookeeper</h2>
                    <img width="100%" src="img/multi/zookeeper-download.png">
                    <aside class="notes">
                        Based on the zookeeper image of .
                    </aside>
                </section>
                <section>
                    <h2>Cluster: DockerFile Zookeeper</h2>
                    <img width="100%" src="img/multi/zookeeper-volume.png">
                </section>
                <section>
                    <h2>Cluster: DockerFile Zookeeper</h2>
                     <img width="100%" src="img/multi/zookeeper-copy-scripts.png">
                </section>
                <section>
                    <h2>Cluster: Startup Scripts Zookeeper</h2>
                     <img width="100%" src="img/multi/zookeeper-configure.png">
                </section>
                <section>
                    <h2>Cluster: Startup Scripts Zookeeper</h2>
                     <img width="100%" src="img/multi/zookeeper-start-script.png">
                </section>
                <section>
                    <h2>Cluster: DockerFile Kafka</h2>
                    <img width="100%" src="img/multi/kafka-download-smaller.png">
                </section>
                <section>
                    <h2>Cluster: DockerFile Kafka</h2>
                    <img width="100%" src="img/multi/kafka-dockerfile.png">
                </section>
                <section>
                    <h2>Cluster: Startup Scripts Kafka</h2>
                    <img width="100%" src="img/multi/kafka-set-props.png">
                </section>
                <section>
                    <h2>Cluster: Startup Scripts Kafka</h2>
                    <img width="100%" src="img/multi/kafka-update-props.png">
                </section>
                <section>
                    <h2>Cluster: Startup Scripts Kafka</h2>
                    <img width="100%" src="img/multi/kafka-start.png">
                </section>
                <section>
                    <h2>Cluster: Docker Compose</h2>
                    <img width="100%" src="img/multi/docker-compose.png">
                </section>
                <section>
                    <h2>Cluster: Docker Compose</h2>
                    <img width="100%" src="img/multi/docker-compose-zookeeper.png">
                </section>
                <section>
                    <h2>Cluster: Docker Compose</h2>
                    <img width="100%" src="img/multi/docker-compose-kafka.png">
                </section>
                <section>
                    <h2>Cluster: Get Started</h2>
                    <p>Get the KAFKA_ADVERTISED_HOSTNAME</p>
                     <pre><code data-trim contenteditable>
                     > ipconfig getifaddr en0
                     </code></pre>
                    <pre><code data-trim contenteditable>
                     > docker-machine ip default
                     </code></pre>
                </section>
                <section>
                    <h2>Cluster: Get Started</h2>
                     <pre><code data-trim contenteditable>
                     > docker-compose up -d
                     </code></pre>
                    <p>Scale up</p>
                    <pre><code data-trim contenteditable>
                     > docker-compose up -d --scale kafka=4
                     </code></pre>
                </section>
                <section>
                    <h2>Cluster: Topics</h2>
                    <p>1 container with both Zookeeper and Kafka</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --create --zookeeper localhost:2181 
                    --replication-factor 4 --partitions 4 --topic test
                     </code></pre>
                    <p>Retrieve a list of topics on Zookeeper</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --list --zookeeper localhost:2181
                     </code></pre>
                    <p>Describe a topic: leader / replicated nodes</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-topics.sh --describe --zookeeper localhost:2181
                     </code></pre>
                </section>
                <section>
                    <h2>Cluster: Publish and Subscribe</h2>
                    <p>Producer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-producer.sh --broker-list="localhost:9092" --topic test
This is a message
This is another message
                     </code></pre>
                    <p>Consumer</p>
                    <pre><code data-trim contenteditable>
                    > bin/kafka-console-consumer.sh --bootstrap-server="localhost:9092" 
                    --topic test --from-beginning
                     </code></pre>
                     <aside class="notes">
                        You can pass more then 1 kafka url in the broker-list / bootstrap server.
                        So that if one of those would be down you can still connect to it
                    </aside>
                </section>
                <section>
                    <h2>Cluster: Result</h2>
                    <ul>
                        <li>Zookeeper Cluster with 3 Nodes</li>
                        <li>Kafka can be scaled on the fly</li>
                        <li>Zookeeper Data retained</li>
                    </ul>
                </section>
                <section>
                    <h2>Cluster: Result - but</h2>
                    <ul>
                        <li>Kafka Nodes get new broker id during startup</li>
                        <li>Zookeeper can not relink topics with Kafka Brokers</li>
                        <li>Can be mitigated with the --no-recreate option of Docker Compose</li>
                    </ul>
                </section>
            </section>

            <section>
                <section>
                    <h2>Kafka Monitor</h2>

                </section>
                <section>
                    <h2>Kafka Monitor: Get Started</h2>
                    <pre><code data-trim contenteditable>
                        $ git clone https://github.com/linkedin/kafka-monitor.git
$ cd kafka-monitor 
$ ./gradlew jar
                     </code></pre>
                    <p>Update the properties if needed in kafka-monitor.properties</p>
                    <pre><code data-trim contenteditable>"zookeeper.connect": "localhost:2181",
                     </code></pre>
                    <pre><code>"bootstrap.servers": "192.168.0.117:32828",
                     </code></pre>
                </section>
                <section>
                    <h2>Kafka Monitor: Get Started</h2>
                    <p>Start it up</p>
                    <pre><code data-trim contenteditable>
                        ./bin/kafka-monitor-start.sh config/kafka-monitor.properties
                     </code></pre>
                </section>
                <section>
                    <h2>Kafka Monitor: Get Started</h2>
                    <p>Go to http://localhost:8000/index.html</p>
                    <img width="100%" src="img/monitor/monitor.png">
                </section>
            </section>
    
            <section>
                <section>
                    <h2>Demo</h2>

                </section>
                <section>
                    <h2>Demo: Topic</h2>
                    
                    <img width="100%" src="img/demo/create-topic.png">
                </section>
                <section>
                    <h2>Demo: Topic</h2>
                    
                    <img width="100%" src="img/demo/describe-topic.png">
                </section>
                <section>
                    <h2>Demo: Publish and Subscribe</h2>
                    
                    <img width="100%" src="img/demo/consume-produce.png">
                </section>
                <section>
                    <h2>Demo: Do It Yourself</h2>
                    <p>https://github.com/tomvdbulck/kafka-docker</p>
                </section>
            </section>

    
    

            <section>
                <section>
                    <h2>Questions ?</h2>

                </section>
            </section>




            <!-- **********
                DO NOT REMOVE
            ********** -->
            <section style="text-align: left;">
                <h2>Thanks for watching!</h2>
                <p class="fragment">Now kick some ass!</p>
            </section>

            <section style="text-align: left;" data-background="img/jworks-wallpaper-3.jpg"></section>
        </div>
    </div>
    <script src="js/reveal.js"></script>
    <script>
        Reveal.initialize({
            transition: 'convex',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                {
                    src: 'lib/js/classList.js',
                    condition: function () {
                        return !document.body.classList;
                    }
                },
                // Interpret Markdown in <section> elements
                {
                    src: 'plugin/markdown/marked.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                {
                    src: 'plugin/markdown/markdown.js',
                    condition: function() {
                        return !!document.querySelector( '[data-markdown]' );
                    }
                },
                // Syntax highlight for <code> elements
                {
                    src: 'plugin/highlight/highlight.js',
                    async: true,
                    callback: function () {
                        hljs.initHighlightingOnLoad();
                    }
                },
                // Zoom in and out with Alt+click
                {
                    src: 'plugin/zoom-js/zoom.js',
                    async: true
                },
                // Speaker notes
                {
                    src: 'plugin/notes/notes.js',
                    async: true
                }
        ]
        });
    </script>
</body>

</html>
